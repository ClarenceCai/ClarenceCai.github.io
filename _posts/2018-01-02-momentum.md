---
layout: post
title: Momentum
---

Last time professor asked me whether I know momentum or not. I said I have no idea what that is. But I know it now.

# Momentum
---
There are 2 articles helping me understand the concept of **momentum** in neural network:

[Improving the way neural networks learn](http://neuralnetworksanddeeplearning.com/chap3.html)
>Stochastic gradient descent by backpropagation has served us well in attacking the MNIST digit classification problem. However, there are many other approaches to optimizing the cost function, and sometimes those other approaches offer performance superior to mini-batch stochastic gradient descent.

Momentum-based gradient descent is one of the variations on stochastic gradient descent.

[Why Momentum Really Works](https://distill.pub/2017/momentum/) 
Momentum dampens oscillations and speeds up the iterations, leads to faster convergence. It has the virtue of spped that gradient descent does not have.

>However, for many problems, plain stochastic gradient descent works well, especially if momentum is used, and so we'll stick to stochastic gradient descent through the remainder of this book.

I will work on it later, after finishing visualizing the deep learning process.
